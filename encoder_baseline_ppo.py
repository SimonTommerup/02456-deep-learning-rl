# -*- coding: utf-8 -*-
"""Encoder_Baseline_PPO.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v1fPPSsbTfRJc2kfoAn_Dg-q-ZFZmlIo

# Getting started with PPO and ProcGen

Here's a bit of code that should help you get started on your projects.

The cell below installs `procgen` and downloads a small `utils.py` script that contains some utility functions. You may want to inspect the file for more details.
"""

#!pip install procgen
#!wget https://raw.githubusercontent.com/nicklashansen/ppo-procgen-utils/main/utils.py

"""Hyperparameters. These values should be a good starting point. You can modify them later once you have a working implementation."""

# Hyperparameters
total_steps = 8e6

num_envs = 32
num_levels = 100
num_steps = 256
num_epochs = 3
n_features = 256
batch_size = 512
eps = .2
grad_eps = .5
value_coef = .5
entropy_coef = .01
lr=5e-4
use_backgrounds = False


# Baseline results
mean_rew_baseline = [3.75, 4.46875, 4.3125, 4.25, 4.71875, 4.09375, 6.375, 4.46875, 4.25, 5.34375, 5.5, 6.84375, 8.0, 7.25, 6.15625, 7.9375, 8.15625, 8.40625, 7.8125, 8.1875, 7.625, 9.6875, 8.8125, 8.3125, 9.9375, 10.2812, 8.375, 9.78125, 10.5312, 8.59375, 9.96875, 10.1562, 9.625, 9.8125, 9.71875, 8.5625, 11.125, 12.4687, 11.4062, 10.4687, 10.2187, 11.5937, 9.9375, 11.75, 12.0625, 11.375, 12.8125, 11.25, 13.4687, 11.875, 12.2812, 11.5937, 13.0625, 11.9375, 13.125, 12.7812, 11.8437, 11.8437, 13.0937, 13.2187, 14.0, 12.5625, 11.625, 13.0625, 13.4687, 12.6875, 12.125, 13.5, 13.9062, 12.4062, 14.1875, 12.9687, 13.4375, 13.1875, 14.4062, 14.4062, 14.3125, 13.25, 12.6562, 13.7187, 13.25, 13.875, 13.375, 13.3437, 12.2812, 14.4062, 14.5312, 14.125, 15.1562, 13.1875, 14.4375, 14.25, 13.9687, 13.8437, 14.1875, 14.875, 14.1875, 15.0312, 14.0, 15.8437, 14.75, 14.4687, 14.0625, 14.3125, 14.7187, 14.8437, 13.125, 14.5937, 15.2812, 13.875, 14.1875, 14.9062, 13.9062, 13.125, 15.125, 13.7812, 14.5625, 13.2812, 13.3125, 14.9687, 13.9375, 15.4687, 13.625, 15.7187, 14.9062, 14.7187, 14.1562, 14.4375, 14.2187, 13.9687, 16.0, 15.1562, 16.0937, 14.5312, 15.9062, 14.2812, 16.4062, 15.6875, 16.375, 14.8125, 14.75, 15.625, 15.9687, 15.1562, 15.125, 16.3437, 15.75, 16.8437, 15.7812, 16.4062, 15.6562, 15.9687, 16.375, 16.0625, 15.0625, 15.4062, 16.0625, 15.875, 16.1875, 16.0937, 16.3437, 15.75, 17.2812, 16.8437, 15.75, 16.2812, 14.8125, 15.75, 16.4687, 17.875, 18.25, 16.0625, 15.1562, 15.9687, 17.875, 15.9687, 16.1562, 18.0625, 17.625, 16.75, 16.3437, 17.1562, 16.0937, 17.0625, 16.2187, 18.0937, 17.1562, 17.125, 16.125, 17.9687, 16.7812, 17.4062, 16.5937, 16.5625, 17.0312, 16.625, 16.3437, 16.9687, 15.8125, 16.3437, 15.9062, 16.0312, 17.5625, 15.9375, 18.0937, 17.1562, 16.7187, 17.4375, 14.9687, 16.3437, 16.125, 17.0937, 16.0, 16.25, 17.7187, 16.1562, 15.5312, 16.5625, 16.7812, 17.3125, 17.4062, 18.875, 14.3437, 17.375, 16.8125, 16.3125, 17.1562, 18.375, 17.75, 18.2187, 16.875, 17.6562, 17.3437, 17.9687, 18.5312, 16.8125, 17.75, 17.2187, 16.3125, 17.3125, 18.7812, 17.0, 17.9062, 15.375, 16.5312, 16.4062, 16.2187, 17.5312, 16.9687, 17.3125, 17.0312, 16.5625, 17.1562, 16.7187, 18.0625, 17.0937, 19.0625, 17.1875, 16.5625, 17.75, 16.75, 16.3437, 17.0625, 16.875, 16.5, 17.0937, 18.2187, 16.1875, 16.7187, 16.875, 17.7812, 16.2812, 15.9375, 17.6875, 16.9062, 17.375, 16.375, 17.3437, 17.9687, 16.8125, 18.1562, 17.7812, 17.7187, 18.2187, 17.5, 17.5937, 17.875, 17.625, 16.625, 18.25, 17.4375, 17.4062, 18.2812, 17.9687, 18.2187, 18.0312, 17.1875, 15.2812, 18.5, 18.0, 16.625, 17.625, 17.4687, 16.9375, 16.8125, 17.3125, 16.875, 17.6562, 17.4375, 17.875, 17.375, 16.2187, 17.2812, 17.125, 17.8125, 17.0, 17.4375, 18.7812, 16.6875, 18.25, 18.0, 18.625, 19.0625, 18.0937, 18.5937, 16.7187, 16.5625, 17.5625, 15.1875, 17.5625, 17.1562, 18.3125, 17.1875, 16.7812, 17.9375, 15.6875, 16.8437, 15.9375, 18.0937, 16.9062, 17.2812, 16.5625, 16.4687, 16.8437, 15.75, 16.5625, 17.0625, 18.2187, 16.9687, 17.375, 16.0625, 18.1875, 18.375, 17.6562, 17.5312, 15.5625, 17.125, 18.8125, 17.3125, 16.125, 16.5937, 18.0, 16.5, 18.5937, 17.8125, 17.1875, 18.7812, 16.8437, 16.2812, 17.875, 15.6875, 18.0937, 18.0937, 15.8125, 19.375, 19.1875, 16.0312, 17.625, 17.5312, 17.1875, 16.2812, 16.875, 17.375, 17.0937, 18.0937, 19.4375]
step_baseline = [8192, 16384, 24576, 32768, 40960, 49152, 57344, 65536, 73728, 81920, 90112, 98304, 106496, 114688, 122880, 131072, 139264, 147456, 155648, 163840, 172032, 180224, 188416, 196608, 204800, 212992, 221184, 229376, 237568, 245760, 253952, 262144, 270336, 278528, 286720, 294912, 303104, 311296, 319488, 327680, 335872, 344064, 352256, 360448, 368640, 376832, 385024, 393216, 401408, 409600, 417792, 425984, 434176, 442368, 450560, 458752, 466944, 475136, 483328, 491520, 499712, 507904, 516096, 524288, 532480, 540672, 548864, 557056, 565248, 573440, 581632, 589824, 598016, 606208, 614400, 622592, 630784, 638976, 647168, 655360, 663552, 671744, 679936, 688128, 696320, 704512, 712704, 720896, 729088, 737280, 745472, 753664, 761856, 770048, 778240, 786432, 794624, 802816, 811008, 819200, 827392, 835584, 843776, 851968, 860160, 868352, 876544, 884736, 892928, 901120, 909312, 917504, 925696, 933888, 942080, 950272, 958464, 966656, 974848, 983040, 991232, 999424, 1007616, 1015808, 1024000, 1032192, 1040384, 1048576, 1056768, 1064960, 1073152, 1081344, 1089536, 1097728, 1105920, 1114112, 1122304, 1130496, 1138688, 1146880, 1155072, 1163264, 1171456, 1179648, 1187840, 1196032, 1204224, 1212416, 1220608, 1228800, 1236992, 1245184, 1253376, 1261568, 1269760, 1277952, 1286144, 1294336, 1302528, 1310720, 1318912, 1327104, 1335296, 1343488, 1351680, 1359872, 1368064, 1376256, 1384448, 1392640, 1400832, 1409024, 1417216, 1425408, 1433600, 1441792, 1449984, 1458176, 1466368, 1474560, 1482752, 1490944, 1499136, 1507328, 1515520, 1523712, 1531904, 1540096, 1548288, 1556480, 1564672, 1572864, 1581056, 1589248, 1597440, 1605632, 1613824, 1622016, 1630208, 1638400, 1646592, 1654784, 1662976, 1671168, 1679360, 1687552, 1695744, 1703936, 1712128, 1720320, 1728512, 1736704, 1744896, 1753088, 1761280, 1769472, 1777664, 1785856, 1794048, 1802240, 1810432, 1818624, 1826816, 1835008, 1843200, 1851392, 1859584, 1867776, 1875968, 1884160, 1892352, 1900544, 1908736, 1916928, 1925120, 1933312, 1941504, 1949696, 1957888, 1966080, 1974272, 1982464, 1990656, 1998848, 2007040, 2015232, 2023424, 2031616, 2039808, 2048000, 2056192, 2064384, 2072576, 2080768, 2088960, 2097152, 2105344, 2113536, 2121728, 2129920, 2138112, 2146304, 2154496, 2162688, 2170880, 2179072, 2187264, 2195456, 2203648, 2211840, 2220032, 2228224, 2236416, 2244608, 2252800, 2260992, 2269184, 2277376, 2285568, 2293760, 2301952, 2310144, 2318336, 2326528, 2334720, 2342912, 2351104, 2359296, 2367488, 2375680, 2383872, 2392064, 2400256, 2408448, 2416640, 2424832, 2433024, 2441216, 2449408, 2457600, 2465792, 2473984, 2482176, 2490368, 2498560, 2506752, 2514944, 2523136, 2531328, 2539520, 2547712, 2555904, 2564096, 2572288, 2580480, 2588672, 2596864, 2605056, 2613248, 2621440, 2629632, 2637824, 2646016, 2654208, 2662400, 2670592, 2678784, 2686976, 2695168, 2703360, 2711552, 2719744, 2727936, 2736128, 2744320, 2752512, 2760704, 2768896, 2777088, 2785280, 2793472, 2801664, 2809856, 2818048, 2826240, 2834432, 2842624, 2850816, 2859008, 2867200, 2875392, 2883584, 2891776, 2899968, 2908160, 2916352, 2924544, 2932736, 2940928, 2949120, 2957312, 2965504, 2973696, 2981888, 2990080, 2998272, 3006464, 3014656, 3022848, 3031040, 3039232, 3047424, 3055616, 3063808, 3072000, 3080192, 3088384, 3096576, 3104768, 3112960, 3121152, 3129344, 3137536, 3145728, 3153920, 3162112]

# Impala training n_features=100
train_impala_n_feauters_100 = [4.875, 5.125, 4.875, 5.9375, 6.875, 6.625, 5.5, 5.3125, 5.4375, 5.375, 6.25, 5.625, 6.25, 7.5, 7.1875, 8.6875, 6.0, 7.0, 8.0625, 6.375, 7.3125, 9.4375, 7.4375, 8.1875, 8.625, 8.5, 8.5, 8.0, 7.5, 9.6875, 8.625, 9.3125, 8.8125, 9.625, 10.0625, 9.0625, 8.6875, 10.25, 7.5625, 7.6875, 7.5, 10.125, 7.6875, 10.25, 10.4375, 10.5625, 10.3125, 9.0625, 10.75, 8.75, 9.9375, 9.9375, 9.6875, 8.875, 8.5625, 10.375, 7.0, 9.0625, 10.8125, 8.6875, 8.9375, 10.375, 9.5625, 10.5625, 10.25, 10.875, 9.125, 11.375, 11.875, 9.875, 9.5625, 11.5, 9.625, 11.0625, 10.9375, 10.5625, 9.25, 10.25, 10.1875, 9.625, 8.6875, 9.0, 9.625, 11.1875, 11.1875, 10.8125, 11.75, 8.25, 10.25, 9.625, 12.0625, 10.1875, 9.9375, 9.375, 10.0625, 8.375, 8.8125, 10.3125, 8.6875, 10.25, 9.625, 9.8125, 12.375, 10.25, 8.875, 12.4375, 11.5, 8.625, 9.0, 8.1875, 8.5, 9.375, 9.8125, 11.875, 12.5, 7.0625, 8.6875, 9.0, 10.75, 11.5, 9.25, 7.375, 9.5, 8.0625, 9.3125, 10.625, 10.125, 9.375, 9.375, 9.1875, 8.375, 10.625, 8.9375, 9.875, 9.875, 8.5, 8.5625, 9.375, 10.0, 9.5625, 9.75, 12.0625, 9.375, 10.8125, 9.5, 8.625, 10.25, 11.5, 7.8125, 10.125, 9.625, 9.75, 8.875, 9.9375, 9.375, 10.1875, 7.9375, 9.0, 10.8125, 7.75, 8.375, 9.4375, 8.4375, 9.3125, 8.875, 9.5625, 10.1875, 8.5625, 8.6875, 8.375, 11.0, 9.8125, 10.3125, 10.0625, 9.6875, 8.5, 11.625, 8.75, 9.6875, 9.5, 9.3125, 9.25, 7.5, 8.75, 8.625, 8.375, 10.0, 8.4375, 7.5625, 10.3125, 8.9375, 8.75, 9.25, 8.75, 9.8125, 8.5625, 8.75, 7.375, 8.375, 9.3125, 10.3125, 10.1875, 9.0, 8.125, 8.9375, 7.0, 8.8125, 7.25, 9.3125]

# Impala n-features=256, n_levels=100
train = [4.75, 4.75, 3.6875, 6.8125, 6.875, 6.0625, 6.0625, 5.625, 6.25, 7.3125, 6.375, 5.5625, 7.25, 6.625, 6.375, 6.0625, 7.25, 5.375, 7.375, 7.375, 7.875, 7.3125, 9.375, 8.25, 9.0625, 6.75, 7.125, 8.9375, 7.9375, 5.75, 6.6875, 7.0, 7.8125, 9.8125, 6.5, 6.625, 7.4375, 6.375, 7.5625, 7.75, 7.75, 9.6875, 8.1875, 8.875, 8.9375, 8.0625, 6.375, 8.375, 5.625, 6.3125, 6.6875, 7.4375, 6.75, 9.5625, 7.125, 5.8125, 7.5625, 7.75, 7.625, 6.5, 6.4375, 7.75, 6.0, 3.625, 6.0625, 5.375, 5.125, 7.3125, 5.6875, 5.0625, 6.75, 7.1875, 7.6875, 8.0625, 5.75, 5.5, 7.5, 6.25, 6.0, 7.8125, 5.375, 5.8125, 6.1875, 6.9375, 7.875, 6.6875, 6.25, 8.375, 7.125, 7.3125, 7.625, 5.9375, 7.0625, 8.5625, 8.25, 7.8125, 5.25, 6.625, 8.25, 7.875, 8.5, 4.4375, 7.375, 5.8125, 8.0625, 7.3125, 5.5625, 6.25, 5.9375, 5.8125, 6.0, 6.75, 7.25, 7.0625, 7.875, 7.25, 6.5, 5.5625, 6.6875, 5.8125, 6.25, 6.625, 7.5, 6.125, 6.625, 6.875, 6.25, 6.9375, 6.375, 7.3125, 8.0625, 6.6875, 5.5625, 7.875, 6.5625, 6.25, 6.9375, 8.0, 7.5, 7.625, 5.5, 6.8125, 6.75, 8.4375, 8.625, 10.125, 10.875, 8.625, 7.375, 7.9375, 8.1875, 7.5625, 7.5625, 9.0, 7.5, 6.0625, 7.1875, 8.0, 7.3125, 7.875, 5.4375, 7.3125, 8.125, 7.125, 6.875, 8.25, 7.0, 9.125, 7.125, 8.1875, 7.25, 6.9375, 8.5625, 7.5625, 9.0625, 8.4375, 8.0625, 6.5625, 7.6875, 7.25, 7.75, 8.0625, 8.6875, 8.125, 9.0625, 9.75, 8.0625, 6.9375, 8.8125, 9.3125, 7.4375, 6.0, 8.5, 8.375, 7.875, 7.1875, 6.5625, 9.6875, 9.625, 9.1875, 7.6875, 9.875, 7.5, 8.5, 8.0, 8.0625, 8.1875, 7.875, 7.0, 5.75, 8.4375, 8.0, 7.75, 8.4375, 5.0, 6.25, 8.1875, 6.9375, 8.125, 6.125, 8.125, 8.3125, 7.875, 9.5, 6.75, 8.6875, 6.6875, 8.1875, 7.1875, 9.25, 4.9375, 7.1875, 8.125, 6.5625, 7.5625, 7.875, 8.0625, 9.5625, 5.875, 8.4375, 7.75, 6.0, 8.9375, 6.625, 6.625, 8.875, 7.5625, 6.1875, 6.5625, 8.375, 7.25, 8.5625, 7.9375, 8.0625, 6.25, 8.3125, 8.8125, 6.9375, 7.75, 8.375, 7.9375, 7.375, 10.75, 7.9375, 7.75, 6.6875, 9.6875, 6.875, 10.0625, 9.1875, 7.8125, 6.5, 7.625, 8.875, 8.125, 8.3125, 7.625, 6.625, 8.5625, 7.5625, 7.125, 8.3125, 5.1875, 8.0625, 8.6875, 7.125, 5.0, 7.4375, 7.375, 5.6875, 4.6875, 8.25, 7.9375, 5.375, 7.5625, 8.375, 8.25, 5.8125, 6.25, 7.625, 7.9375, 7.125, 6.5, 8.0625, 8.5, 8.3125, 7.375, 6.5625, 9.0, 8.6875, 6.6875, 7.75, 7.25, 5.8125, 8.5625, 7.625, 8.3125, 6.5, 6.6875, 8.9375, 8.4375, 7.6875, 6.375, 7.8125, 7.8125, 6.625, 7.5625, 6.4375, 6.625, 5.5, 6.8125, 8.8125, 8.0, 6.75, 7.1875, 8.4375, 6.1875, 8.0, 9.0625, 5.375, 6.1875, 7.3125, 8.625, 7.6875, 7.1875, 6.375, 7.1875, 7.5625, 8.5625, 7.0, 7.4375, 8.3125, 7.0625, 9.5625, 7.125, 9.6875, 6.25, 9.4375, 9.3125, 4.0, 6.75, 7.5, 7.375, 5.1875, 7.4375, 6.6875, 6.75, 6.5, 8.125, 8.25, 7.25, 7.875, 8.6875, 7.625, 9.3125, 7.3125, 9.5625, 7.5625, 8.125, 8.375, 8.875, 7.5625, 8.5, 8.625, 9.375, 8.875, 9.0625, 8.8125, 7.1875, 9.5625, 7.5625, 7.9375, 8.25, 7.375, 8.125, 8.0, 8.5, 7.4375, 6.6875, 6.5625, 9.9375, 7.5625, 8.75, 7.75, 7.25, 8.875, 7.25, 7.6875, 7.0625, 8.625, 7.375, 7.0, 9.125, 7.3125, 6.6875, 6.5625, 6.8125, 8.3125, 8.8125, 8.875, 8.8125, 9.375, 8.75, 8.75, 9.0, 6.4375, 8.3125, 7.25, 8.0, 9.375, 8.0, 6.5625, 6.4375, 6.75, 5.875, 7.1875, 7.5625, 6.5, 7.6875, 9.875, 7.25, 8.5625, 7.4375, 9.375, 7.6875, 7.75, 6.8125, 7.5625, 7.5625, 8.0, 7.6875, 7.8125, 7.75, 7.8125, 8.0, 7.875, 7.6875, 8.5625, 8.3125, 7.625, 7.75, 8.0625, 7.875, 8.5625, 6.9375, 8.9375, 9.0625, 7.875, 9.0625, 8.6875, 9.0, 9.3125, 9.125, 7.0625, 8.9375, 10.75, 7.875, 8.125, 9.5625, 9.3125, 9.625, 8.1875, 8.75, 10.0, 10.0, 10.5625, 8.375, 8.1875, 8.8125, 8.1875, 6.9375, 9.125, 8.9375, 8.8125, 8.625, 7.75, 8.1875, 8.4375, 7.5625, 7.4375, 10.3125, 7.0625, 5.625, 8.1875, 8.875, 10.875, 9.3125, 9.4375, 7.375, 8.0625, 7.375, 7.75, 9.4375, 8.8125, 9.0625, 9.75, 9.625, 8.125, 8.375, 10.5, 9.875, 9.375, 7.375, 10.8125, 8.1875, 6.9375, 9.0, 7.5625, 8.75, 9.125, 10.0625, 8.125, 7.9375, 7.75, 9.5, 10.0, 9.625, 10.1875, 10.8125, 7.9375, 8.875, 8.625, 8.6875, 7.1875, 8.3125, 8.5, 9.25, 8.625, 9.0, 7.8125, 7.25, 8.375, 9.0625, 10.5625, 8.5625, 10.125, 9.75, 11.25, 8.1875, 9.3125, 10.75, 9.4375, 6.75, 8.5, 12.625, 9.3125, 10.25, 10.125, 7.5625, 8.375, 10.875, 9.375, 9.0625, 7.9375, 9.75, 7.75, 8.125, 8.875, 10.0, 7.5, 9.1875, 7.3125, 8.4375, 7.8125, 8.75, 9.8125, 8.375, 8.4375, 9.6875, 7.4375, 9.9375, 8.0625, 7.6875, 9.5, 8.25, 6.8125, 8.8125, 8.0, 9.5625, 8.0, 8.625, 7.875, 8.625, 7.875, 8.25, 7.8125, 7.25, 8.8125, 11.0625, 9.9375, 8.8125, 9.25, 9.25, 9.75, 9.25, 8.3125, 10.3125, 9.25, 8.8125, 8.3125, 10.375, 9.0, 8.25, 8.9375, 9.375, 11.5625, 12.3125, 9.75, 8.5625, 8.9375, 12.25, 8.9375, 7.8125, 9.6875, 10.4375, 9.25, 8.6875, 7.25, 8.625, 8.8125, 9.4375, 8.625, 8.9375, 10.8125, 11.4375, 10.3125, 9.0, 8.375, 7.5, 8.25, 9.9375, 9.0, 10.5, 8.375, 6.8125, 10.1875, 10.25, 9.25, 9.1875, 9.25, 7.75, 8.875, 9.9375, 8.625, 9.5, 9.125, 10.5, 8.625, 7.9375, 9.625, 9.3125, 8.125, 8.875, 9.875, 8.9375, 9.75, 7.5625, 10.4375, 8.125, 10.25, 7.9375, 8.6875, 10.25, 7.9375, 9.5, 7.875, 9.3125, 10.125, 7.25, 9.5, 8.0, 10.0625, 10.625, 10.0625, 8.75, 9.375, 9.125, 10.875, 9.5, 9.4375, 10.25, 8.875, 8.25, 9.5, 11.25, 9.25, 11.5, 8.3125, 8.5, 9.3125, 10.9375, 9.4375, 7.8125, 11.0, 10.8125, 9.1875, 7.0, 10.625, 10.5625, 6.5625, 7.375, 9.6875, 7.6875, 7.0625, 8.625, 8.625, 7.5625, 11.5625, 10.4375, 9.8125, 7.5625, 11.875, 10.1875]
val = [7.73458194732666, 5.886229515075684, 4.710787773132324, 5.243834495544434, 6.220511436462402, 5.443582057952881, 5.239720821380615, 6.213399410247803, 4.438948631286621, 5.4878411293029785, 5.5414910316467285, 4.477890968322754, 3.7003912925720215, 6.086633205413818, 5.7361369132995605, 5.739877223968506, 6.152451992034912, 6.832823276519775, 5.584362983703613, 5.800239086151123, 7.350476264953613, 5.072155952453613, 6.238905429840088, 7.734626293182373, 5.329860210418701, 7.106696128845215, 7.200695991516113, 5.99720573425293, 5.95735502243042, 3.72792911529541, 5.874333381652832, 4.888392448425293, 4.338999271392822, 4.962979316711426, 5.283779621124268, 4.569885730743408, 5.578627586364746, 5.917409420013428, 4.883570671081543, 7.536930561065674, 5.866125106811523, 6.493924617767334, 5.611307621002197, 5.932555198669434, 6.056929588317871, 5.810683250427246, 5.8983378410339355, 4.313194751739502, 6.132729530334473, 5.697922706604004, 6.1066155433654785, 6.104576110839844, 5.07056999206543, 5.518230438232422, 6.871774673461914, 4.923274517059326, 5.514261722564697, 5.254212379455566, 4.255971908569336, 5.42103385925293, 4.0421576499938965, 4.38306999206543, 3.8859493732452393, 3.575709819793701, 4.7994465827941895, 2.802946090698242, 3.097411870956421, 2.9713833332061768, 3.220963478088379, 3.1843419075012207, 3.294132709503174, 3.547917127609253, 3.9357821941375732, 2.7461400032043457, 3.1901614665985107, 4.121758937835693, 3.889065742492676, 3.803069829940796, 3.6208698749542236, 3.192925214767456, 4.091893672943115, 2.9674034118652344, 4.167080402374268, 3.1365303993225098, 2.746852397918701, 3.356419086456299, 3.569596290588379, 5.03486442565918, 4.737323760986328, 3.940131664276123, 3.9987239837646484, 4.452108860015869, 5.009712219238281, 3.8965725898742676, 3.6470110416412354, 4.87148380279541, 5.334678649902344, 5.730259895324707, 3.7031540870666504, 3.9663171768188477, 5.281822204589844, 3.860079050064087, 3.871511697769165, 4.591598033905029, 4.589646816253662, 4.085868835449219, 3.2653040885925293, 3.8838353157043457, 3.1246957778930664, 3.6453089714050293, 4.8284912109375, 4.1663289070129395, 5.712134838104248, 3.503994941711426, 4.991845607757568, 4.364781379699707, 4.4218926429748535, 3.9641077518463135, 6.175687313079834, 4.422124862670898, 4.996109962463379, 5.616285800933838, 4.4729814529418945, 4.582064628601074, 2.8359568119049072, 4.9050612449646, 2.7373204231262207, 5.276635646820068, 4.710446357727051, 5.168585777282715, 3.9777188301086426, 5.78843879699707, 4.018867492675781, 6.129184246063232, 5.960709571838379, 5.033692836761475, 4.521598815917969, 4.316442489624023, 4.5245137214660645, 4.1162848472595215, 3.3969736099243164, 8.028796195983887, 4.357326507568359, 5.995875358581543, 5.226363658905029, 5.637552738189697, 5.478099822998047, 5.368642330169678, 5.51915168762207, 5.770257949829102, 4.695419788360596, 6.167497634887695, 4.518834590911865, 4.269740104675293, 4.22373104095459, 3.0553107261657715, 4.741235733032227, 3.368813991546631, 3.630845785140991, 5.119930744171143, 5.120989799499512, 4.351080894470215, 3.3809568881988525, 3.7452778816223145, 4.3136773109436035, 5.442234516143799, 4.6205244064331055, 6.058756351470947, 5.286737442016602, 5.898224830627441, 4.922029972076416, 4.768670082092285, 6.054965972900391, 3.6421079635620117, 4.054005146026611, 4.974306106567383, 5.128799915313721, 5.028807163238525, 5.388422012329102, 5.699338912963867, 4.262633323669434, 5.136667728424072, 5.235618591308594, 3.594143867492676, 4.007092475891113, 4.211258888244629, 4.57191276550293, 5.238267421722412, 4.982177734375, 5.752933025360107, 5.844933032989502, 4.82031774520874, 4.0552778244018555, 4.517342567443848, 3.9051246643066406, 6.576652526855469, 6.005728721618652, 5.022807598114014, 5.535322189331055, 6.353000164031982, 5.683110237121582, 6.905007839202881, 5.924291610717773, 5.205421447753906, 4.7452802658081055, 6.735384941101074, 6.168623924255371, 5.603793621063232, 4.635263442993164, 5.5533552169799805, 4.227756977081299, 6.622611999511719, 6.924622058868408, 5.54385232925415, 5.845305919647217, 4.212491512298584, 5.376376152038574, 5.174317359924316, 6.134489059448242, 4.310133934020996, 3.807939291000366, 4.622610092163086, 5.235746383666992, 4.525933265686035, 5.394749164581299, 5.648128986358643, 4.831932544708252, 4.4237494468688965, 4.121492385864258, 3.9722962379455566, 4.843682289123535, 3.977578639984131, 5.764096736907959, 4.844429016113281, 4.899286270141602, 5.463423252105713, 5.056344032287598, 4.904074192047119, 4.907170295715332, 4.293430328369141, 4.955202102661133, 4.904824256896973, 5.927990913391113, 4.396018028259277, 5.265780448913574, 4.141672611236572, 5.062707424163818, 5.882694244384766, 4.399598598480225, 4.2995710372924805, 4.503652095794678, 4.709053993225098, 5.120250701904297, 5.122959613800049, 5.7377705574035645, 4.915093421936035, 4.816143989562988, 5.5319623947143555, 5.122689247131348, 5.892280578613281, 5.580620765686035, 5.679906845092773, 5.574639320373535, 4.601953029632568, 4.550845146179199, 5.673952579498291, 6.541221618652344, 4.137494087219238, 5.622817516326904, 4.9577717781066895, 4.752572059631348, 4.804814338684082, 4.292034149169922, 4.397598743438721, 4.60190486907959, 5.2129974365234375, 4.753975868225098, 4.295119285583496, 4.910599708557129, 3.9911510944366455, 4.148181915283203, 4.712819576263428, 4.969526767730713, 5.890321731567383, 4.713662624359131, 5.687009811401367, 4.345937728881836, 4.398316860198975, 3.7874486446380615, 3.839320421218872, 5.120450973510742, 4.147367477416992, 5.174097537994385, 4.868366241455078, 4.664850234985352, 5.435059547424316, 5.283023834228516, 5.387706279754639, 4.979504108428955, 3.7499732971191406, 5.088542461395264, 5.29216194152832, 3.8041629791259766, 4.321541786193848, 4.889303207397461, 4.480876445770264, 5.1019673347473145, 3.9187610149383545, 4.695641040802002, 4.95442533493042, 5.369181156158447, 5.109978675842285, 5.110581398010254, 4.803478240966797, 4.3411335945129395, 4.603216171264648, 5.585859298706055, 5.583919525146484, 5.168999671936035, 4.9129815101623535, 6.102914333343506, 5.173586845397949, 4.863708972930908, 5.021674156188965, 5.6982622146606445, 6.113080978393555, 4.7147722244262695, 4.976723670959473, 5.445240020751953, 5.601694583892822, 5.084331512451172, 5.708887577056885, 5.244656085968018, 5.55598258972168, 4.7786784172058105, 6.02863883972168, 4.885879039764404, 3.847804546356201, 3.746152877807617, 4.529465198516846, 5.363839626312256, 4.218548774719238, 6.3031110763549805, 5.732929706573486, 5.159512519836426, 6.825087070465088, 5.052228927612305, 5.052278995513916, 4.89797306060791, 4.535250186920166, 5.005252361297607, 5.578063011169434, 5.16317081451416, 3.5994460582733154, 4.7507710456848145, 5.326803207397461, 5.798290252685547, 4.4384765625, 5.793961524963379, 4.228541851043701, 4.33413028717041, 5.067233562469482, 5.536081790924072, 4.439406871795654, 5.2246928215026855, 6.375275611877441, 4.755414009094238, 6.216034889221191, 4.230834007263184, 4.756195545196533, 5.488014221191406, 5.59259557723999, 6.377003192901611, 3.919130802154541, 6.271954536437988, 6.005852699279785, 5.587811470031738, 4.806008815765381, 5.641562461853027, 5.170772552490234, 4.9102373123168945, 6.216043949127197, 5.587012767791748, 5.5873565673828125, 5.483532905578613, 5.953061580657959, 5.850440502166748, 4.754178047180176, 4.966080665588379, 5.123063564300537, 4.444711208343506, 5.386601448059082, 5.282188892364502, 3.9762115478515625, 4.3452653884887695, 5.130940914154053, 5.028115272521973, 3.0399765968322754, 4.71981143951416, 4.19774055480957, 4.830150127410889, 4.779932022094727, 5.150444030761719, 4.888750076293945, 4.83809757232666, 3.9455204010009766, 5.63043212890625, 4.525135517120361, 5.315194606781006, 6.36774206161499, 4.524083614349365, 4.73617696762085, 5.577792167663574, 5.1051716804504395, 6.211021900177002, 6.631293296813965, 4.6319661140441895, 5.001922607421875, 5.317159175872803, 5.685778617858887, 6.316258907318115, 5.683063507080078, 5.525383949279785, 6.364518165588379, 4.52360725402832, 5.419769287109375, 5.472561359405518, 4.684425354003906, 4.634338855743408, 5.215554237365723, 5.7419281005859375, 5.1082868576049805, 6.056384563446045, 6.580682754516602, 6.893024921417236, 5.419218063354492, 5.997785568237305, 6.04796838760376, 5.155354022979736, 5.41779899597168, 5.943935394287109, 5.944769859313965, 4.63067102432251, 4.893378257751465, 5.100229740142822, 6.467441558837891, 5.098865032196045, 3.9963414669036865, 4.103123188018799, 5.419782638549805, 6.523674488067627, 5.784941673278809, 5.153263092041016, 5.470526695251465, 4.523671627044678, 7.519223213195801, 4.779726505279541, 5.305484771728516, 4.4659624099731445, 5.93771505355835, 6.619121551513672, 5.618597030639648, 7.031627178192139, 5.50737190246582, 7.290314674377441, 6.973509311676025, 6.395841598510742, 6.446695327758789, 5.870702743530273, 5.872467041015625, 6.604064464569092, 6.183161735534668, 6.077617645263672, 4.715445041656494, 6.129136085510254, 6.021668910980225, 5.600674629211426, 7.483681678771973, 7.423886299133301, 7.155120849609375, 6.785031318664551, 5.009631156921387, 6.106558322906494, 6.366454601287842, 6.051837921142578, 5.424805641174316, 5.945344924926758, 6.20591402053833, 5.946057319641113, 6.099032402038574, 5.731948375701904, 5.469333648681641, 5.052080154418945, 5.886037349700928, 6.612218379974365, 5.9823317527771, 6.131902694702148, 6.022092342376709, 6.537734508514404, 6.06633186340332, 5.338737487792969, 4.717787742614746, 6.791619777679443, 7.088147163391113, 5.738812446594238, 7.910089015960693, 5.324062824249268, 6.771399974822998, 6.303383827209473, 6.094833850860596, 5.886455535888672, 6.091527462005615, 6.605008125305176, 7.528937339782715, 5.871189594268799, 5.765113353729248, 5.711371421813965, 6.4810872077941895, 6.16842794418335, 6.727786540985107, 6.466098785400391, 7.590175628662109, 5.483097076416016, 6.1492815017700195, 5.733503341674805, 5.219475746154785, 5.986883640289307, 6.08811616897583, 7.105769157409668, 7.200783729553223, 5.970695495605469, 6.630249500274658, 6.577577114105225, 6.064937591552734, 6.266571044921875, 5.908446311950684, 6.264556407928467, 5.194025039672852, 6.16103458404541, 6.0580267906188965, 6.970226764678955, 6.560922622680664, 6.252253532409668, 7.264321804046631, 6.294983863830566, 7.658695220947266, 6.0809502601623535, 5.873366832733154, 5.56810188293457, 4.7585368156433105, 6.830245494842529, 6.269703388214111, 7.276269912719727, 6.309115409851074, 6.708611965179443, 6.8067426681518555, 7.102167129516602, 5.081963539123535, 6.135529518127441, 5.730100154876709, 5.780741214752197, 5.378627777099609, 5.480214595794678, 7.588688373565674, 5.973158836364746, 4.567173004150391, 6.523964881896973, 5.970524787902832, 4.816484451293945, 5.619668483734131, 5.9681549072265625, 6.264737606048584, 5.359961032867432, 4.257759094238281, 5.309470176696777, 7.31113862991333, 6.353771686553955, 7.299750804901123, 6.245944976806641, 6.542569160461426, 6.586452007293701, 8.074592590332031, 6.324045181274414, 6.6709184646606445, 6.7639594078063965, 6.761916637420654, 6.209606647491455, 7.048128604888916, 6.002978324890137, 6.695182800292969, 5.453470230102539, 6.540597915649414, 5.147871494293213, 5.294368743896484, 6.528720855712891, 5.6869893074035645, 6.476431846618652, 4.7447404861450195, 7.0658721923828125, 5.877909183502197, 5.580783843994141, 5.285510063171387, 6.963428974151611, 5.726135730743408, 6.613066673278809, 6.0668625831604, 6.213969707489014, 6.163410663604736, 6.261745452880859, 7.145118713378906, 6.100425720214844, 6.689711570739746, 5.70182991027832, 5.798130989074707, 4.421763896942139, 5.845959186553955, 3.9788742065429688, 5.7473955154418945, 6.774745941162109, 5.9865593910217285, 7.160582542419434, 7.007691860198975, 6.365765571594238, 5.334961891174316, 5.92072868347168, 5.624408721923828, 6.503800868988037, 6.206021308898926, 6.496050834655762, 6.1029791831970215, 7.271040916442871, 6.145990371704102, 6.972743034362793, 6.4327616691589355, 6.379605293273926, 6.180462837219238, 6.080845832824707, 6.2244038581848145, 6.462797164916992, 6.216238498687744, 6.4527812004089355, 6.449493408203125, 7.2195281982421875, 6.680756568908691, 6.435365676879883, 4.788692474365234, 4.787316799163818, 5.753775119781494, 4.736919403076172, 6.715725421905518, 5.937483310699463, 7.189628601074219, 8.095480918884277, 6.882303237915039, 6.109889984130859, 5.5306830406188965, 5.434527397155762, 7.4997477531433105, 5.9093427658081055, 6.629260540008545, 7.730890274047852, 5.613299369812012, 6.37965726852417, 5.992661952972412, 5.225631237030029, 6.995022773742676, 7.662321090698242, 6.174617290496826, 6.841437339782715, 6.695169448852539, 5.8330278396606445, 4.829741477966309, 6.885589122772217, 7.500842571258545, 6.253885746002197, 8.867976188659668, 6.713021278381348, 5.949249744415283, 7.13535213470459, 6.1310882568359375, 6.6056132316589355, 6.651875972747803, 7.45089054107666, 7.063350677490234, 7.196937561035156, 7.143934726715088, 6.2403883934021, 6.426158905029297, 6.563669204711914, 6.227957248687744, 6.555800437927246, 7.306024551391602, 7.581824779510498, 4.89458703994751, 7.106719017028809, 6.113469123840332, 7.898523807525635, 6.251863479614258, 5.452552795410156, 7.000322341918945, 5.443389892578125, 6.238816261291504, 6.613413333892822, 4.829631805419922, 5.346316337585449, 7.501428127288818, 5.857243537902832, 5.388613224029541, 5.856835842132568, 6.0897016525268555, 4.9191412925720215, 6.091350555419922, 5.9962687492370605, 5.525609970092773, 5.804052352905273, 7.249580383300781, 4.67277193069458, 6.258142471313477, 5.4144439697265625, 5.040242671966553, 6.016122817993164, 4.614149570465088, 5.825793743133545, 6.569218158721924, 5.403421401977539, 5.4027557373046875, 4.797443389892578, 6.70678186416626, 6.193729877471924, 6.050668239593506, 6.093968868255615, 7.298679351806641, 5.389070510864258]

"""Some usefull utilities for working with different models:"""

def xavier_uniform_init(module, gain=1.0):
    if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):
        nn.init.xavier_uniform_(module.weight.data, gain)
        nn.init.constant_(module.bias.data, 0)
    return module

"""Network definitions. We have defined a policy network for you in advance. It uses the popular `NatureDQN` encoder architecture (see below), while policy and value functions are linear projections from the encodings. There is plenty of opportunity to experiment with architectures, so feel free to do that! Perhaps implement the `Impala` encoder from [this paper](https://arxiv.org/pdf/1802.01561.pdf) (perhaps minus the LSTM)."""

import torch
import torch.nn as nn
import torch.nn.functional as F
from utils import make_env, Storage, orthogonal_init
import matplotlib.pyplot as plt
from IPython.display import clear_output

class Flatten(nn.Module):
    def forward(self, x):
        return x.view(x.size(0), -1)

### NATURE DQN ###
class DQNEncoder(nn.Module):
  def __init__(self, in_channels, feature_dim):
    super().__init__()
    self.layers = nn.Sequential(
        nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=8, stride=4), nn.ReLU(),
        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2), nn.ReLU(),
        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1), nn.ReLU(),
        Flatten(),
        nn.Linear(in_features=1024, out_features=feature_dim), nn.ReLU()
    )
    self.apply(orthogonal_init)

  def forward(self, x):
    return self.layers(x)
### NATURE DQN ###


### IMPALA ### 
"""
Implementation inspired by https://github.com/joonleesky/train-procgen-pytorch/blob/master/common/model.py
"""
class ResidualBlock(nn.Module):
    def __init__(self,
                 in_channels):
        super(ResidualBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1)

    def forward(self, x):
        out = nn.ReLU()(x)
        out = self.conv1(out)
        out = nn.ReLU()(out)
        out = self.conv2(out)
        return out + x
  
class ImpalaBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(ImpalaBlock, self).__init__()
        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)
        self.res1 = ResidualBlock(out_channels)
        self.res2 = ResidualBlock(out_channels)

    def forward(self, x):
        x = self.conv(x)
        x = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)(x)
        x = self.res1(x)
        x = self.res2(x)
        return x

class ImpalaModel(nn.Module):
    def __init__(self,
                 in_channels,
                 feature_dim):
        super(ImpalaModel, self).__init__()
        self.block1 = ImpalaBlock(in_channels=in_channels, out_channels=16)
        self.block2 = ImpalaBlock(in_channels=16, out_channels=32)
        self.block3 = ImpalaBlock(in_channels=32, out_channels=32)
        self.fc = nn.Linear(in_features=32 * 8 * 8, out_features=feature_dim)

        self.output_dim = 256
        self.apply(xavier_uniform_init)

    def forward(self, x):
        x = self.block1(x)
        x = self.block2(x)
        x = self.block3(x)
        x = nn.ReLU()(x)
        x = Flatten()(x)
        x = self.fc(x)
        x = nn.ReLU()(x)
        return x

### IMPALA ###

class Policy(nn.Module):
  def __init__(self, encoder, feature_dim, num_actions):
    super().__init__()
    self.encoder = encoder
    self.policy = orthogonal_init(nn.Linear(feature_dim, num_actions), gain=.01)
    self.value = orthogonal_init(nn.Linear(feature_dim, 1), gain=1.)

  def act(self, x):
    with torch.no_grad():
      x = x.cuda().contiguous()
      dist, value = self.forward(x)
      action = dist.sample()
      log_prob = dist.log_prob(action)
    
    return action.cpu(), log_prob.cpu(), value.cpu()

  def forward(self, x):
    x = self.encoder(x)
    logits = self.policy(x)
    value = self.value(x).squeeze(1)
    dist = torch.distributions.Categorical(logits=logits)

    return dist, value

"""**Define environments, networks and optimizer.**"""

# Define training environment.
env = make_env(num_envs, 
               num_levels=num_levels, 
               use_backgrounds=use_backgrounds)
print('Observation space:', env.observation_space)
print('Action space:', env.action_space.n)

# Define validation environment.
eval_env = make_env(num_envs, 
                    start_level=num_levels, 
                    num_levels=10,  #CHANGED
                    use_backgrounds=use_backgrounds)

# Define networks.
n_features = 256 # TODO: Change to 256 for impala!!
#encoder = DQNEncoder(env.observation_space.shape[0], n_features)
encoder = ImpalaModel(env.observation_space.shape[0], n_features)

policy = Policy(encoder, n_features, env.action_space.n)
policy.cuda()

# Define optimizer.
# These are reasonable values but probably not optimal.
optimizer = torch.optim.Adam(policy.parameters(), lr=lr, eps=1e-5) # CHANGED lr=5e-4, 1e-3

# Define temporary storage.
# We use this to collect transitions during each iteration.
storage = Storage(
    env.observation_space.shape,
    num_steps,
    num_envs
)

# Define name.
name = '4_baseline_Impala_value_clip'


# Save rewards for plotting purposeses.
rewards_train = []
rewards_val = []
steps = []

# Define environments 
obs = env.reset()
step = 0
obs_val = eval_env.reset()
save_step = 5e5

"""**Training**"""

# Run training 

while step < total_steps:

  # Save every half-million step
  if step > save_step:
    torch.save(policy.state_dict, 'temp_model_' + name + '.pt')
    #TODO:
    """ Also save rewards.. plot etc..."""
    save_step += 5e5

  temp_val_rewards = []

  # Use policy to collect data for num_steps steps
  policy.eval()
  for _ in range(num_steps):
    # Use policy
    action, log_prob, value = policy.act(obs)
    action_val, log_prob_val, value_val = policy.act(obs_val)
    
    # Take step in environment
    next_obs, reward, done, info = env.step(action)
    next_obs_val, reward_val, done_val, info_val = eval_env.step(action_val) # CHANGED TO ACTION_VAL FOR ACTION

    # Store data
    storage.store(obs, action, reward, done, info, log_prob, value)
    temp_val_rewards.append(torch.Tensor(reward_val))

    # Update current observation
    obs = next_obs
    obs_val = next_obs_val # CHANGE BY ADDING THIS LINE

  # Add the last observation to collected data
  _, _, value = policy.act(obs)
  storage.store_last(obs, value)

  # Compute return and advantage
  storage.compute_return_advantage()

  # Optimize policy
  policy.train()
  for epoch in range(num_epochs):

    # Iterate over batches of transitions
    generator = storage.get_generator(batch_size)
    for batch in generator:
      b_obs, b_action, b_log_prob, b_value, b_returns, b_advantage = batch

      # Get current policy outputs
      new_dist, new_value = policy(b_obs)
      new_log_prob = new_dist.log_prob(b_action)

      # Clipped policy objective
      ratio = torch.exp(new_log_prob - b_log_prob)
      clipped_ratio = ratio.clamp(min=1.0 - eps, max=1.0 + eps)
      policy_reward = torch.min(ratio*b_advantage, clipped_ratio*b_advantage)
      pi_loss = policy_reward.mean() 

      # Clipped value function objective
      clipped_value = b_value + (new_value - b_value).clamp(min=-eps, max=eps)
      v_surr1 = (new_value - b_returns).pow(2)
      v_surr2 = (clipped_value - b_returns).pow(2)
      value_loss = 0.5 * torch.max(v_surr1, v_surr2).mean()

      # clipped_value = (new_value - b_value)**2
      # value_loss = clipped_value.mean()

      # Entropy loss
      entropy_loss = new_dist.entropy().mean()

      # Backpropagate losses
      loss = -pi_loss  + value_coef*value_loss - entropy_coef*entropy_loss # Minimize loss <=> maximize objective (notice sign opposed to paper)
      loss.backward()

      # Clip gradients
      torch.nn.utils.clip_grad_norm_(policy.parameters(), grad_eps)

      # Update policy
      optimizer.step()
      optimizer.zero_grad()

  # Update stats
  step += num_envs * num_steps
  
  # Save rewards for plots.
  rewards_train.append(storage.get_reward(normalized_reward=False))
  rewards_val.append(torch.stack(temp_val_rewards).mean(1).sum(0))
  steps.append(step)

  # Plot training and validation reward vs baseline.
  clear_output(wait=True)
  plt.subplots(figsize=(10,6))
  plt.plot(step_baseline, mean_rew_baseline, 
           color="darksalmon", label="Baseline")
  plt.plot(steps, rewards_train, color="darkslategray", 
           label='Training reward: ' + str(rewards_train[-1].item()))
  plt.plot(steps, rewards_val, color="darkolivegreen", 
           label='Validation reward: ' + str(rewards_val[-1].item()))
  plt.xlabel("steps")
  plt.ylabel("Mean reward")
  plt.legend()
  plt.show()

# Save final plot.
plt.savefig('plot_' + name + '.pdf', bbox_inches='tight')

print('Completed training!')
torch.save(policy.state_dict, 'model_' + name + '.pt')
print("Results are saved in variables \'rewards\' and \'steps\'. Print these and copy them into cell above, if we wish to save this checkpoint")

"""Below cell can be used for policy evaluation and saves an episode to mp4 for you to view."""

import imageio
from utils import make_env, Storage, orthogonal_init
from tqdm import tqdm

# Make evaluation environment
# eval_env = make_env(num_envs, start_level=num_levels, num_levels=num_levels, use_backgrounds=use_backgrounds)
obs = eval_env.reset()

frames = []
total_reward = []

# Evaluate policy
policy.eval()

"""
for i in range(self.n_envs):
            for j in range(steps):
                self.episode_rewards[i].append(rew_batch[i][j])
                if done_batch[i][j]:
                    self.episode_len_buffer.append(len(self.episode_rewards[i]))
                    self.episode_reward_buffer.append(np.sum(self.episode_rewards[i]))
                    self.episode_rewards[i] = []
                    self.num_episodes += 1
"""
                    
for _ in tqdm(range(512)):

  # Use policy
  action, log_prob, value = policy.act(obs)

  # Take step in environment
  obs, reward, done, info = eval_env.step(action)
  total_reward.append(torch.Tensor(reward))  

  # Render environment and store
  frame = (torch.Tensor(eval_env.render(mode='rgb_array'))*255.).byte()
  frames.append(frame)

# Calculate average return
total_reward = torch.stack(total_reward).sum(0).mean(0)
print('Average return:', total_reward)

# Save frames as video
frames = torch.stack(frames)
imageio.mimsave('vid_' + name + '.mp4', frames, fps=25)

imageio.mimsave('vid_' + "name" + '.mp4', frames, fps=25)

