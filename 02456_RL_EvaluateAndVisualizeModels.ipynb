{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02456 RL: EvaluateAndVisualizeModels.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9m7yBbgeNU6"
      },
      "source": [
        "## Visualizing Improvements of Reinforcement Learning Models\r\n",
        "\r\n",
        "This is a notebook accompagnying the Github repository https://github.com/SimonTommerup/02456-deep-learning-rl. It contains a demonstration of an evaluation loop of one of the models as well as a recording of the model playing with added saliency maps. \r\n",
        "\r\n",
        "\r\n",
        "First to run this notebook, you will have to enable a GPU run-time in Google Colab under Run Time > Change Run Time Type. \r\n",
        "\r\n",
        "The first code cell clones into our Github repository and installs procgen. The output is currently suppressed by `%%capture`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sb7wenAUMgA5"
      },
      "source": [
        "%%capture\r\n",
        "!git clone https://github.com/SimonTommerup/02456-deep-learning-rl.git\r\n",
        "!pip install procgen"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdTGbpRIGqoe"
      },
      "source": [
        "Next we will import a number of dependencies as well as some modules from the repository: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXYIn5QpQRxh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32d6044b-5177-4299-ffe8-f7e8895b67ff"
      },
      "source": [
        "import os\r\n",
        "import torch\r\n",
        "import imageio\r\n",
        "from tqdm import tqdm\r\n",
        "os.chdir(\"02456-deep-learning-rl/src\")\r\n",
        "import models\r\n",
        "import saliency\r\n",
        "import utils\r\n",
        "print(f\"Current working directory: {os.getcwd()}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current working directory: /content/02456-deep-learning-rl/src\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jj1yu_vzGxmM"
      },
      "source": [
        "The helper function `get_settings` is useful to avoid being confused by having two sets of parameter declarations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8CAWUmVQbwA"
      },
      "source": [
        "def get_settings(num_envs=1, env_name=\"starpilot\", start_level=1, num_levels=1, num_features=256, use_backgrounds=False):\r\n",
        "  settings = {}\r\n",
        "  settings[\"env_name\"] = env_name\r\n",
        "  settings[\"num_envs\"] = num_envs\r\n",
        "  settings[\"start_level\"] = start_level\r\n",
        "  settings[\"num_levels\"] = num_levels\r\n",
        "  settings[\"num_features\"] = num_features\r\n",
        "  settings[\"use_backgrounds\"] = use_backgrounds\r\n",
        "  return settings"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6RX4aL9HJAG"
      },
      "source": [
        "The function `get_env_model_hook` returns a loaded model in evaluation mode, an environment with some specified settings and a hook which is used for recording. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WOM2Q2PfZHz"
      },
      "source": [
        "def get_env_model_hook(model_folder, encoder_name, settings):\r\n",
        "  # create env\r\n",
        "  env = utils.make_env(settings[\"num_envs\"], \r\n",
        "                       env_name=settings[\"env_name\"], \r\n",
        "                       start_level=settings[\"start_level\"], \r\n",
        "                       num_levels=settings[\"num_levels\"], \r\n",
        "                       use_backgrounds=settings[\"use_backgrounds\"])\r\n",
        "\r\n",
        "  # set correct encoder\r\n",
        "  assert encoder_name in [\"impala\", \"dqn\"], \"Encoder must be either impala or dqn\"\r\n",
        "  if encoder_name == \"impala\":\r\n",
        "    encoder = models.ImpalaModel(env.observation_space.shape[0], settings[\"num_features\"])\r\n",
        "  elif encoder_name == \"dqn\":\r\n",
        "    encoder = models.DQNEncoder(env.observation_space.shape[0], settings[\"num_features\"])\r\n",
        "\r\n",
        "  policy = models.Policy(encoder, settings[\"num_features\"], env.action_space.n)\r\n",
        "\r\n",
        "  #load tuned parameters and set to eval\r\n",
        "  model_path = saliency.get_full_path(model_folder)\r\n",
        "  policy.load_state_dict(torch.load(model_path))\r\n",
        "  policy.cuda()\r\n",
        "  policy.eval()\r\n",
        "\r\n",
        "  # create hook\r\n",
        "  hook = saliency.PolicyLogitsHook(policy)\r\n",
        "\r\n",
        "  return [env, policy, hook]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2TkjIHsHbYj"
      },
      "source": [
        "In the next code cell we will specify which model to look at. This is model experiment `5` trained for `500_lvls` (i.e 500 levels) with the encoder `impala` and using `valclip` i.e (validation clipping). \r\n",
        "\r\n",
        "To try out other models, an explanation of the naming convention is provided in the readme at https://github.com/SimonTommerup/02456-deep-learning-rl under the subsection \"Overview of experiments\". \r\n",
        "\r\n",
        "After having selected another model to look at simply change the variable `model_folder` by selecting a folder name in the folder `experiments`. \r\n",
        "\r\n",
        "E.g. you could set `model_folder = 11_model_5_bigfish`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iX7zSsQojJzl"
      },
      "source": [
        "# NOTE: \r\n",
        "# Choose the right encoder:\r\n",
        "# MODEL 2 = DQN\r\n",
        "# MODEL 5 = Impala\r\n",
        "\r\n",
        "model_folder = \"5_500_lvls_impala_valclip\"\r\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkfFV_VRMvBl"
      },
      "source": [
        "The function `evaluation` evaluates a specified model. Our results was created with `num_envs = 32` and `num_levels = 10`. Further in order to evaluate it should be kept in mind that since we trained on 500 levels the starting level for evaluation should be at least 501. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMzxl04RMtvE"
      },
      "source": [
        "def evaluation(model_folder, env_model_hook, no_steps, settings):\r\n",
        "  # Initialize\r\n",
        "  frames = []\r\n",
        "  rewards = []\r\n",
        "  env = env_model_hook[0]\r\n",
        "  policy = env_model_hook[1]\r\n",
        "\r\n",
        "  storage = utils.Storage(env.observation_space.shape, no_steps, settings[\"num_envs\"], gamma=0.99)\r\n",
        "\r\n",
        "  obs = env.reset()\r\n",
        "  for _ in tqdm(range(no_steps)):\r\n",
        "    # Use policy on observation on frame\r\n",
        "    action, log_prob, value = policy.act(obs)\r\n",
        "\r\n",
        "    # Take step in environment\r\n",
        "    next_obs,reward,done,info = env.step(action)\r\n",
        "\r\n",
        "    # Save reward\r\n",
        "    storage.store(obs, action, reward, done, info, log_prob, value)\r\n",
        "\r\n",
        "    # update current observation\r\n",
        "    obs = next_obs\r\n",
        "  \r\n",
        "  validation_reward = storage.get_reward(normalized_reward=True)\r\n",
        "  print(f\"Validation reward: {validation_reward}\")\r\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjLZ8jfuNWdk"
      },
      "source": [
        "The followed code cell executes a validation of a model trained for 500 levels on `starpilot` using the `impala`-encoder and validation clipping:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOET6iApNCiR",
        "outputId": "65e87f5e-99f4-48f1-8b50-6e03a6bd4eb5"
      },
      "source": [
        "no_steps=256\r\n",
        "validation_settings = get_settings(num_envs=32, start_level=501, num_levels=10)\r\n",
        "val_env_model_hook = get_env_model_hook(model_folder, \"impala\", validation_settings)\r\n",
        "evaluation(model_folder, val_env_model_hook, no_steps, validation_settings)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 256/256 [00:15<00:00, 16.59it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation reward: 17.34375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cthMSncPJMKC"
      },
      "source": [
        "The function `recording` makes a recording of a specified model with some specified environment settings. To make the video the number of environments should be set to the value 1. The parameter `no_steps` controls the number of steps or frames to be recorded. Also it should be kept in mind that a model trained on e.g. `starpilot` is also recorded with the parameter `env_name` set to `starpilot`. \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-DPGYTY4LJQ"
      },
      "source": [
        "def recording(model_folder, env_model_hook, no_steps, settings):\r\n",
        "  # Initialize\r\n",
        "  frames = []\r\n",
        "  rewards = []\r\n",
        "  env = env_model_hook[0]\r\n",
        "  policy = env_model_hook[1]\r\n",
        "  hook = env_model_hook[2]\r\n",
        "\r\n",
        "  obs = env.reset()\r\n",
        "  for _ in tqdm(range(no_steps)):\r\n",
        "    # Use policy on observation on frame\r\n",
        "    action,_,_ = policy.act(obs)\r\n",
        "\r\n",
        "    # Get logits\r\n",
        "    logits = hook.get_logits()\r\n",
        "\r\n",
        "    # Get saliency\r\n",
        "    sf = saliency.saliency_frame(net=policy, hook=hook, logits=logits, frame=obs, pixel_step=4)\r\n",
        "\r\n",
        "    # Set saliency mode\r\n",
        "    mode = \"max\"\r\n",
        "    sf = saliency.saliency_mode(sf, mode=mode)\r\n",
        "\r\n",
        "    # Rendering\r\n",
        "    frame = env.render(mode=\"rgb_array\")\r\n",
        "\r\n",
        "    constant = 200\r\n",
        "    sigma = 5\r\n",
        "    channel = saliency.color_to_channel(\"red\")\r\n",
        "    frame = saliency.saliency_on_procgen(frame, sf, channel=channel, constant=constant, sigma=sigma)\r\n",
        "\r\n",
        "    # Record frame to frames stack\r\n",
        "    frame = (torch.Tensor(frame)).byte()\r\n",
        "    frames.append(frame)\r\n",
        "\r\n",
        "    # Take step in environment\r\n",
        "    obs,reward,_,_ = env.step(action)\r\n",
        "\r\n",
        "  frames = torch.stack(frames)\r\n",
        "\r\n",
        "  start_level = settings[\"start_level\"]\r\n",
        "  env_name = settings[\"env_name\"]\r\n",
        "  video_path = env_name + \"_\" + model_folder + \"_\" + f\"level_played={start_level}\" + \"_\" + f\"c={constant}_\" + f\"sig={sigma}_\"+ f\"mode={mode}\" + \".mp4\"\r\n",
        "  print(f\"Saving video to {video_path}\")\r\n",
        "  imageio.mimsave(video_path, frames, fps=5)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzhDcHCkN-rz"
      },
      "source": [
        "The recorded movie can subsequently be found in the current working directory (should be `/content/02456-deep-learning-rl/src`, else see the output of code cell 2 above) in .mp4 format and you should be able to download this file and watch it.\r\n",
        "\r\n",
        "The following code block records a movie of a model trained for 500 levels on `starpilot` using the `impala`-encoder and validation clipping:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKBoq1616kh_",
        "outputId": "9c9eef28-6a31-40cc-d2bd-ad159d0e7845"
      },
      "source": [
        "import warnings\r\n",
        "warnings.filterwarnings(\"ignore\") # warning on nn.Upsample acknowledged & suppressed\r\n",
        "\r\n",
        "# Set number of steps (equivalent to seen frames).\r\n",
        "# The video is set to be recorded with 5 frames per second.\r\n",
        "no_steps = 256\r\n",
        "record_settings = get_settings()\r\n",
        "record_env_model_hook = get_env_model_hook(model_folder, \"impala\", record_settings)\r\n",
        "recording(model_folder, record_env_model_hook, no_steps, record_settings)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 256/256 [11:12<00:00,  2.63s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saving video to starpilot_5_500_lvls_impala_valclip_level_played=1_c=200_sig=5_mode=max.mp4\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}